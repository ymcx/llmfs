{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, pipeline\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from accelerate import Accelerator\n",
    "from typing import defaultdict\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eafb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train[:1%]\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation[:1%]\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,\n",
    "        \"valid\": ds_valid,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def pair_freqs(self,tokens):\n",
    "        freqs = defaultdict(int)\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(len(tokens[i][0])-1):\n",
    "                pair = (tokens[i][0][j], tokens[i][0][j+1])\n",
    "                \n",
    "                freqs[pair] += tokens[i][1]\n",
    "        return sorted(freqs.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "            \n",
    "    def merge_tokens(self, tokens, pair):\n",
    "        tokens_new = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            \n",
    "            w = []\n",
    "            \n",
    "            j = 0\n",
    "            while j < len(tokens[i][0]) - 1:\n",
    "                \n",
    "                pair_new = (tokens[i][0][j], tokens[i][0][j+1])\n",
    "                \n",
    "                if pair == pair_new:\n",
    "                    t = tokens[i][0][j] + tokens[i][0][j+1]\n",
    "                    \n",
    "                    w.append(t)\n",
    "                    j += 2\n",
    "                else:\n",
    "                    w.append(tokens[i][0][j])\n",
    "                    j += 1\n",
    "            \n",
    "            if j == len(tokens[i][0])-1:\n",
    "                w.append(tokens[i][0][j])\n",
    "                \n",
    "                \n",
    "            tokens_new.append( (w, tokens[i][1]) )\n",
    "            i += 1\n",
    "            \n",
    "        return tokens_new\n",
    "    def __len__(self):\n",
    "        return len(vocab)\n",
    "    def __init__(self,corpus:list[str], vocab_size,max_len):\n",
    "        self.tokenizerz = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "        word_freqs = defaultdict(int)\n",
    "        tokens = []\n",
    "        vocab = set()\n",
    "        vocab.add('<|pad|>')\n",
    "        vocab.add('<|bos|>')\n",
    "        vocab.add('<|eos|>')\n",
    "        vocab.add('<|unk|>')\n",
    "        \n",
    "        \n",
    "        for text in corpus:\n",
    "            words = self.tokenizerz._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            words = [word for word, offset in words]\n",
    "            \n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "                \n",
    "                for char in word:\n",
    "                    vocab.add(char)\n",
    "               \n",
    "        chars = len(vocab)\n",
    "            \n",
    "        for k,v in word_freqs.items():\n",
    "            tokens.append( ([c for c in k], v) )\n",
    "            \n",
    "        rules = []\n",
    "            \n",
    "        while len(vocab) < vocab_size:\n",
    "            pair = self.pair_freqs(tokens)\n",
    "            rules.append(pair)\n",
    "            \n",
    "            vocab.add(f\"{pair[0]}{pair[1]}\")\n",
    "            tokens = self.merge_tokens(tokens, pair)\n",
    "        \n",
    "        self.rules = rules\n",
    "        self.vocab=vocab\n",
    "        self.tokens = tokens\n",
    "        \n",
    "        vocab = list(vocab)\n",
    "        \n",
    "        self.tok2id = {vocab[i]: i+4 for i in range(len(vocab))}\n",
    "        self.id2tok = {i+4: vocab[i] for i in range(len(vocab))}\n",
    "        \n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.unk_token_id = 3\n",
    "\n",
    "        self.pad_token = '<|pad|>'\n",
    "        self.bos_token = '<|bos|>'\n",
    "        self.eos_token = '<|eos|>'\n",
    "        self.unk_token = '<|unk|>'\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def tokenize(self,text):\n",
    "        words = self.tokenizerz._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        text = [c for word, _ in words for c in word]\n",
    "        \n",
    "        for rule in self.rules:\n",
    "            newtext = []\n",
    "            i = 0\n",
    "            \n",
    "            while (i < len(text) - 1) and (len(newtext) < self.max_len):\n",
    "                if (text[i],text[i+1]) == rule:\n",
    "                    newtext.append(f\"{rule[0]}{rule[1]}\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    newtext.append(text[i])\n",
    "                i += 1\n",
    "                        \n",
    "            if (i == len(text)-1) and (len(newtext) < self.max_len):\n",
    "                newtext.append(text[i])\n",
    "                \n",
    "            text = newtext\n",
    "        \n",
    "        ids = [self.tok2id[c] if c in self.tok2id else 3 for c in text] + [0] * (self.max_len - len(text))\n",
    "        mask = [1] * len(text)                                          + [0] * (self.max_len - len(text))\n",
    "        return ids, mask\n",
    "        \n",
    "\n",
    "    def __call__(self,texts):\n",
    "        if isinstance(texts, str):\n",
    "            ids,mask = self.tokenize(texts)\n",
    "            return {\n",
    "                \"input_ids\": ids,\n",
    "                \"attention_mask\": mask,\n",
    "            }\n",
    "        else:\n",
    "            texts = [self.tokenize(text) for text in texts]\n",
    "            return {\n",
    "                \"input_ids\": [ids for ids,_ in texts],\n",
    "                \"attention_mask\": [mask for _,mask in texts] \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "\n",
    "corpset = list(raw_datasets[\"valid\"][\"content\"])\n",
    "tokenizer = Tokenizer(corpset,2000,context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e393a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    \"\"\"\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\"\"\"\n",
    "    outputs = tokenizer(element[\"content\"])\n",
    "    return outputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keytoken_weighted_loss(inputs, logits, alpha=1.0):\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184189b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=bs, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"valid\"], batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather_for_metrics(outputs.loss))\n",
    "        \n",
    "    loss = torch.mean(torch.tensor(losses))\n",
    "    \n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f257466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 50\n",
    "completed_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53467282",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "progress = tqdm.tqdm(total=num_training_steps)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        for i in range(len(batch[\"input_ids\"])):\n",
    "            if (len(batch[\"input_ids\"][i]) != context_length):\n",
    "                print(i)\n",
    "                print(context_length)\n",
    "                print(batch[\"input_ids\"][i])\n",
    "                print(batch[\"attention_mask\"][i])\n",
    "                print()\n",
    "                print(batch)\n",
    "                \n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_sched.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"import\"\n",
    "for _ in range(20):\n",
    "    toks = tokenizer(text)\n",
    "    logits = model(**toks).logits\n",
    "    best_id = torch.argmax(logits, dim=-1)\n",
    "    text += tokenizer.id2tok[best_id]\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
