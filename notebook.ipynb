{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, pipeline\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from accelerate import Accelerator\n",
    "from typing import defaultdict\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547678e-2b44-4639-959a-a7dfa261493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNN = 100\n",
    "epochsN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return self.vocab_id[ids]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens).replace(\"Ä \", \" \")\n",
    "\n",
    "    def pair_freqs(self, tokens):\n",
    "        freqs = defaultdict(int)\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(len(tokens[i][0]) - 1):\n",
    "                pair = (tokens[i][0][j], tokens[i][0][j + 1])\n",
    "\n",
    "                freqs[pair] += tokens[i][1]\n",
    "        return sorted(freqs.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "\n",
    "    def merge_tokens(self, tokens, pair):\n",
    "        tokens_new = []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            w = []\n",
    "\n",
    "            j = 0\n",
    "            while j < len(tokens[i][0]) - 1:\n",
    "                pair_new = (tokens[i][0][j], tokens[i][0][j + 1])\n",
    "\n",
    "                if pair == pair_new:\n",
    "                    t = tokens[i][0][j] + tokens[i][0][j + 1]\n",
    "\n",
    "                    w.append(t)\n",
    "                    j += 2\n",
    "                else:\n",
    "                    w.append(tokens[i][0][j])\n",
    "                    j += 1\n",
    "\n",
    "            if j == len(tokens[i][0]) - 1:\n",
    "                w.append(tokens[i][0][j])\n",
    "\n",
    "            tokens_new.append((w, tokens[i][1]))\n",
    "            i += 1\n",
    "\n",
    "        return tokens_new\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(vocab)\n",
    "\n",
    "    def __init__(self, corpus: list[str], vocab_size, max_len):\n",
    "        self.pre_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")._tokenizer.pre_tokenizer\n",
    "\n",
    "        word_freqs = defaultdict(int)\n",
    "        tokens = []\n",
    "        vocab = {\"<|pad|>\", \"<|unk|>\"}\n",
    "\n",
    "        for text in corpus:\n",
    "            words = self.pre_tokenizer.pre_tokenize_str(text)\n",
    "            words = [word for word, offset in words]\n",
    "\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "\n",
    "                for char in word:\n",
    "                    vocab.add(char)\n",
    "                    \n",
    "        vocab = list(vocab)\n",
    "\n",
    "        chars = len(vocab)\n",
    "\n",
    "        for k, v in word_freqs.items():\n",
    "            tokens.append(([c for c in k], v))\n",
    "\n",
    "        rules = []\n",
    "        while len(vocab) < vocab_size:\n",
    "            pair = self.pair_freqs(tokens)\n",
    "            rules.append(pair)\n",
    "\n",
    "            vocab.add(f\"{pair[0]}{pair[1]}\")\n",
    "            tokens = self.merge_tokens(tokens, pair)\n",
    "\n",
    "        self.rules = rules\n",
    "        self.tokens = tokens\n",
    "        \n",
    "        self.vocab = {vocab[i]: i for i in range(len(vocab))}\n",
    "        self.vocab_id = {b:a for a,b in self.vocab.items()}\n",
    "\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "\n",
    "        self.pad_token = \"<|pad|>\"\n",
    "        self.unk_token = \"<|unk|>\"\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        texts,\n",
    "        truncation=False,\n",
    "        max_length=None,\n",
    "        return_tensors=None,\n",
    "        padding=None,\n",
    "    ):\n",
    "        text = \" \".join(texts)\n",
    "        text = [\n",
    "            char\n",
    "            for word, _ in self.pre_tokenizer.pre_tokenize_str(text)\n",
    "            for char in word\n",
    "        ]\n",
    "\n",
    "        for rule in self.rules:\n",
    "            newtext = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(text) - 1:\n",
    "                if (text[i], text[i + 1]) == rule:\n",
    "                    newtext.append(f\"{rule[0]}{rule[1]}\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    newtext.append(text[i])\n",
    "                i += 1\n",
    "\n",
    "            if i == len(text) - 1:\n",
    "                newtext.append(text[i])\n",
    "\n",
    "            text = newtext\n",
    "\n",
    "        n = self.max_len\n",
    "        ids = [self.vocab[c] if c in self.vocab else 1 for c in text]\n",
    "        \n",
    "        ids = [\n",
    "            ids[i : i + n]\n",
    "            for i in range(0, len(ids), n)\n",
    "            if not truncation or i + n <= len(ids)\n",
    "        ]\n",
    "\n",
    "        mask1 = [[1] * n] * (len(ids) - 1)\n",
    "        mask2 = [[1] * (len(ids[-1])) + [0] * (n - len(ids[-1]))]\n",
    "        mask = mask1 + mask2\n",
    "\n",
    "        ids[-1] = ids[-1] + [0] * (n - len(ids[-1]))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": ids,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"huggingface-course/codeparrot-ds-train\")[\"train\"].shuffle(seed=42).select(range(NNN)),\n",
    "    \"valid\": load_dataset(\"huggingface-course/codeparrot-ds-train\")[\"train\"].shuffle(seed=42).select(range(NNN)),\n",
    "})\n",
    "\n",
    "context_length = 128\n",
    "corpset = list(dataset[\"valid\"][\"content\"])\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "vs = 300\n",
    "tokenizer = Tokenizer(corpset,vs,context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e393a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=context_length\n",
    "    )\n",
    "\n",
    "    return outputs\n",
    "\n",
    "dataset_tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset[\"train\"].column_names)#, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_ctx=context_length,\n",
    "    #bos_token_id=tokenizer.bos_token_id,\n",
    "    #eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184189b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "\n",
    "dataset_tokenized.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True)\n",
    "eval_dataloader = DataLoader(dataset_tokenized[\"valid\"], batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather_for_metrics(outputs.loss))\n",
    "        \n",
    "    loss = torch.mean(torch.tensor(losses))\n",
    "    \n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f257466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = epochsN\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 50\n",
    "completed_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53467282",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_fct = CrossEntropyLoss()\n",
    "progress = tqdm.tqdm(total=num_training_steps)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader, start=1):        \n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        \n",
    "        shift_labels = batch[\"input_ids\"][..., 1:].contiguous()\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "    \n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \"\"\"\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        del logits\n",
    "        del shift_logits\n",
    "        del shift_labels     \n",
    "\n",
    "        \n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_sched.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "\n",
    "            \n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"x = \"\"\"\n",
    "newt = []\n",
    "\n",
    "for _ in range(20):\n",
    "    #toks = tokenizer(text, return_tensors=\"pt\")\n",
    "    #toks.to(\"cuda\")\n",
    "    toks = tokenizer(text, None,None)\n",
    "    toks = {a:torch.tensor(b).to(\"cuda\") for a,b in toks.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**toks).logits\n",
    "    best_id = torch.argmax(logits, dim=-1)[0][0].item()\n",
    "    tok = tokenizer.convert_ids_to_tokens(best_id)\n",
    "    newt.append(tok)\n",
    "\n",
    "text = text  + tokenizer.convert_tokens_to_string(newt)\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
