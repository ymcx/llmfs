{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BufferedReader\n",
    "from zstandard import ZstdDecompressor\n",
    "from urllib import request\n",
    "from datetime import datetime\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.nn import (\n",
    "    Linear,\n",
    "    Embedding,\n",
    "    ModuleList,\n",
    "    Module,\n",
    "    LayerNorm,\n",
    "    GELU,\n",
    "    CrossEntropyLoss,\n",
    "    functional,\n",
    "    utils,\n",
    ")\n",
    "import torch\n",
    "import re\n",
    "import pandas\n",
    "import tarfile\n",
    "import tiktoken\n",
    "import html\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aca821-5116-49cd-9de7-5fe8b3d47c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@saeed.mehrang/understanding-grouped-query-attention-a-practical-guide-with-pytorch-implementation-9e3f9f26bb79\n",
    "class GQA(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.q_proj = Linear(N_EMBD, N_EMBD)\n",
    "        self.k_proj = Linear(N_EMBD, N_EMBD * N_KVHEAD // N_QHEAD)\n",
    "        self.v_proj = Linear(N_EMBD, N_EMBD * N_KVHEAD // N_QHEAD)\n",
    "        self.o_proj = Linear(N_EMBD, N_EMBD)\n",
    "\n",
    "    def forward(self, x, mask, is_causal):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(batch_size, CONTEXT_SIZE, N_QHEAD, N_EMBD // N_QHEAD)\n",
    "        k = k.view(batch_size, CONTEXT_SIZE, N_KVHEAD, N_EMBD // N_QHEAD)\n",
    "        v = v.view(batch_size, CONTEXT_SIZE, N_KVHEAD, N_EMBD // N_QHEAD)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        mask = mask.view(batch_size, 1, 1, CONTEXT_SIZE).expand(\n",
    "            batch_size, 1, CONTEXT_SIZE, CONTEXT_SIZE\n",
    "        )\n",
    "\n",
    "        if is_causal:\n",
    "            mask = mask & torch.ones(\n",
    "                batch_size,\n",
    "                1,\n",
    "                CONTEXT_SIZE,\n",
    "                CONTEXT_SIZE,\n",
    "                dtype=torch.bool,\n",
    "                device=\"cuda\",\n",
    "            ).tril(0)\n",
    "\n",
    "        x = functional.scaled_dot_product_attention(\n",
    "            q, k, v, dropout_p=DROPOUT, attn_mask=mask, enable_gqa=True\n",
    "        )\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, CONTEXT_SIZE, N_EMBD)\n",
    "        x = self.o_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = Linear(N_EMBD, 4 * N_EMBD)\n",
    "        self.gelu = GELU(\"tanh\")\n",
    "        self.c_proj = Linear(4 * N_EMBD, N_EMBD)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = LayerNorm(N_EMBD)\n",
    "        self.attn = GQA()\n",
    "        self.ln_2 = LayerNorm(N_EMBD)\n",
    "        self.mlp = MLP()\n",
    "\n",
    "    def forward(self, x, mask, is_causal):\n",
    "        x = x + self.attn(self.ln_1(x), mask, is_causal)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos = torch.arange(CONTEXT_SIZE, device=\"cuda\")\n",
    "        self.wte = Embedding(vocab_size, N_EMBD)\n",
    "        self.wpe = Embedding(CONTEXT_SIZE, N_EMBD)\n",
    "        self.h = ModuleList(Decoder() for _ in range(N_LAYER))\n",
    "        self.ln_f = LayerNorm(N_EMBD)\n",
    "        self.lm_head = Linear(N_EMBD, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask, is_causal):\n",
    "        x = self.wte(x) + self.wpe(self.pos)\n",
    "\n",
    "        for decoder in self.h:\n",
    "            x = decoder(x, mask, is_causal)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc54f54-3f1d-41d1-bed7-ac07d582b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model):\n",
    "    parameters = [\n",
    "        {\"params\": [], \"weight_decay\": WEIGHT_DECAY},\n",
    "        {\"params\": [], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if any(i in name for i in [\"wpe.weight\", \"bias\", \"ln\"]):\n",
    "            parameters[1][\"params\"].append(parameter)\n",
    "        else:\n",
    "            parameters[0][\"params\"].append(parameter)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d2b84-0e67-422e-812b-70598edb317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    while True:\n",
    "        tokens_and_mask, i = tokenize(text, tokenizer, False)\n",
    "        tokens_and_mask = torch.tensor(tokens_and_mask, device=\"cuda\")\n",
    "\n",
    "        if tokens_and_mask.size(0) != 1:\n",
    "            break\n",
    "\n",
    "        tokens = tokens_and_mask[:, 0, :]\n",
    "        mask = tokens_and_mask[:, 1, :].to(torch.bool)\n",
    "\n",
    "        with torch.autocast(\"cuda\"), torch.no_grad():\n",
    "            logits = model(tokens, mask, False)[0, i - 1, :]\n",
    "\n",
    "        probabilities = torch.softmax(logits, 0)\n",
    "        probabilities, indices = torch.topk(probabilities, TOPK)\n",
    "        index = torch.multinomial(probabilities, 1)\n",
    "        token = indices[index]\n",
    "\n",
    "        if token == tokenizer.eot_token:\n",
    "            break\n",
    "\n",
    "        text += tokenizer.decode(token.tolist())\n",
    "\n",
    "    model.train()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8da146-9576-4500-894e-2757285bf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(batch, loss_function):\n",
    "    references = batch[\"tokens\"][:, 1:]\n",
    "    references = references.contiguous().view(-1)\n",
    "\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        predictions = model(batch[\"tokens\"], batch[\"mask\"], True)\n",
    "\n",
    "    predictions = predictions[:, :-1, :]\n",
    "    predictions = predictions.contiguous().view(-1, predictions.size(2))\n",
    "\n",
    "    return loss_function(predictions, references)\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "\n",
    "    data_loader = itertools.islice(data_loader, 256 // BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        losses = [get_loss(batch, loss_function) for batch in data_loader]\n",
    "        loss = sum(losses) / len(losses)\n",
    "\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_model(model, train, valid, loss_function, optimizer, scheduler, scaler):\n",
    "    def print_information(step):\n",
    "        progress = step * BATCH_SIZE / DATASET_SIZE * 100\n",
    "        time = (datetime.now() - time_start).total_seconds() / 60\n",
    "        train_loss = evaluate_model(model, train, loss_function)\n",
    "        valid_loss = evaluate_model(model, valid, loss_function)\n",
    "        print(f\"{progress:3.0f}% | {time:4.0f} | {train_loss:5.2f} | {valid_loss:5.2f}\")\n",
    "\n",
    "    print(\"prog | time | train | valid\")\n",
    "    time_start = datetime.now()\n",
    "\n",
    "    for step, batch in enumerate(train, 1):\n",
    "        loss = get_loss(batch, loss_function) / ACCUMULATION_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print_information(step)\n",
    "\n",
    "        if step % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            utils.clip_grad_norm_(model.parameters(), MAX_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12054945-ebbb-4aa3-8013-f2911ccf836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    request.urlretrieve(\n",
    "        \"https://zenodo.org/records/3606810/files/pol_0616-1119_labeled.tar.zst\",\n",
    "        \"dataset_train.tar.zst\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Dataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, dataset):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        i = 0\n",
    "        for post in self.read_posts():\n",
    "            if \"com\" not in post:\n",
    "                continue\n",
    "\n",
    "            comment = self.parse_comment(post[\"com\"])\n",
    "            if not comment:\n",
    "                continue\n",
    "\n",
    "            for tokens_and_mask in tokenize(comment, self.tokenizer, True)[0]:\n",
    "                tokens_and_mask = torch.tensor(tokens_and_mask, device=\"cuda\")\n",
    "                tokens = tokens_and_mask[0, :]\n",
    "                mask = tokens_and_mask[1, :].to(torch.bool)\n",
    "\n",
    "                yield {\"tokens\": tokens, \"mask\": mask}\n",
    "\n",
    "                i += 1\n",
    "                if DATASET_SIZE <= i:\n",
    "                    return\n",
    "\n",
    "        print(\"Ran out of dataset items early, LR scheduling might be unexpected\")\n",
    "\n",
    "    def read_posts(self):\n",
    "        with open(self.dataset, \"rb\") as dataset:\n",
    "            with ZstdDecompressor().stream_reader(dataset) as stream:\n",
    "                with tarfile.open(fileobj=BufferedReader(stream), mode=\"r|\") as file:\n",
    "                    for line in file.extractfile(file.firstmember):\n",
    "                        for post in json.loads(line.decode(\"utf-8\"))[\"posts\"]:\n",
    "                            yield post\n",
    "\n",
    "    def parse_comment(self, text):\n",
    "        text = html.unescape(text)\n",
    "        text = re.sub(r\"<br>\", \"\\n\", text)\n",
    "\n",
    "        while re.search(r\"<[^<>]*>\", text):\n",
    "            text = re.sub(r\"<[^<>]*>\", \"\", text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02788b8d-bc50-4af2-9558-daa38bc9e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(data_loader, optimizer):\n",
    "    total_steps = DATASET_SIZE // (BATCH_SIZE * ACCUMULATION_STEPS)\n",
    "    warmup_steps = int(total_steps * WARMUP)\n",
    "    linear = LinearLR(optimizer, start_factor=LEARNING_RATE, total_iters=warmup_steps)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps)\n",
    "\n",
    "    return SequentialLR(\n",
    "        optimizer=optimizer,\n",
    "        schedulers=[linear, cosine],\n",
    "        milestones=[warmup_steps],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d55dc-b01f-435d-838b-2bfafed7db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer, add_eot):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if add_eot:\n",
    "        tokens += [tokenizer.eot_token]\n",
    "\n",
    "    padded = []\n",
    "    length = len(tokens)\n",
    "    tokens_list = [tokens[i : i + CONTEXT_SIZE] for i in range(0, length, CONTEXT_SIZE)]\n",
    "\n",
    "    for tokens in tokens_list:\n",
    "        remaining = CONTEXT_SIZE - len(tokens)\n",
    "        mask = [1] * len(tokens) + [0] * remaining\n",
    "        tokens += [43000] * remaining\n",
    "        padded.append((tokens, mask))\n",
    "\n",
    "    return padded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c17368-f8fe-4d41-8111-a822a45c9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCUMULATION_STEPS = 8\n",
    "BATCH_SIZE = 16\n",
    "BETAS = (0.9, 0.999)\n",
    "CONTEXT_SIZE = 128\n",
    "DATASET_SIZE = 8192\n",
    "DROPOUT = 0.1\n",
    "EPS = 1e-8\n",
    "LEARNING_RATE = 5e-4\n",
    "MAX_NORM = 1.0\n",
    "N_EMBD = 768\n",
    "N_KVHEAD = 3\n",
    "N_LAYER = 12\n",
    "N_QHEAD = 12\n",
    "TOPK = 50\n",
    "WARMUP = 0.01\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89837290-0dcf-4241-bfe2-24bf7e16294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2b640-0f7e-47a0-ba50-2272f3762c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")\n",
    "train = DataLoader(Dataset(tokenizer, \"dataset_train.tar.zst\"), batch_size=BATCH_SIZE)\n",
    "valid = DataLoader(Dataset(tokenizer, \"dataset_valid.tar.zst\"), batch_size=BATCH_SIZE)\n",
    "model = Model(tokenizer.n_vocab).to(\"cuda\")\n",
    "optimizer = AdamW(params=get_parameters(model), lr=LEARNING_RATE, betas=BETAS, eps=EPS, fused=True)\n",
    "scheduler = get_scheduler(train, optimizer)\n",
    "loss_function = CrossEntropyLoss(ignore_index=43000)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a553ec-bb2c-453b-9d7f-fab1bd873891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train, valid, loss_function, optimizer, scheduler, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb7445-e53a-4af6-a2a9-0e5e67611f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(\"There\", model, tokenizer)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
