{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, pipeline\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from accelerate import Accelerator\n",
    "from typing import defaultdict\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9587292-7437-4a59-bb15-24d2f93f07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(\"lol\"[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def get_pair_freqs(self, token_freqs):\n",
    "        pair_freqs = defaultdict(int)\n",
    "        \n",
    "        for token,freq in token_freqs:\n",
    "            for i in range(len(token) - 1):\n",
    "                pair = token[i:i+2]\n",
    "                pair_freqs[pair] += freq\n",
    "                \n",
    "        return sorted(pair_freqs.items(), key=lambda x: x[1])\n",
    "\n",
    "    def merge_tokens(self, token_freqs, pair):\n",
    "        token_freqs_new = []\n",
    "\n",
    "        for token,freq in token_freqs: \n",
    "            w = []\n",
    "\n",
    "            i = 0\n",
    "            while i < len(token) - 1:\n",
    "                pair_new = token[i:i+2]\n",
    "\n",
    "                if pair == pair_new:\n",
    "                    w.append(pair)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    w.append(token[i])\n",
    "                    i += 1\n",
    "\n",
    "            if i == len(token) - 1:\n",
    "                w.append(token[i])\n",
    "\n",
    "            token_freq = (w, freq)\n",
    "            token_freqs_new.append(token_freq)\n",
    "\n",
    "        return token_freqs_new\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(vocab)\n",
    "\n",
    "    def get_word_freqs(self, corpus):\n",
    "        word_freqs = defaultdict(int)\n",
    "        for text in corpus:\n",
    "            for word in text.split():\n",
    "                word_freqs[word] += 1\n",
    "\n",
    "        return word_freqs\n",
    "\n",
    "    def get_tokens(self, word_freqs):\n",
    "        tokens = []\n",
    "        for k, v in word_freqs.items():\n",
    "            tokens.append((k, v))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def make_rules(self, vocab, vocab_size, tokens):\n",
    "        rules = []\n",
    "        while len(vocab) < vocab_size:\n",
    "            pair_freqs = self.get_pair_freqs(tokens)\n",
    "            pair = pair_freqs[-1][0]\n",
    "            rules.append(pair)\n",
    "            vocab.add(f\"{pair[0]}{pair[1]}\")\n",
    "            tokens = self.merge_tokens(tokens, pair)\n",
    "        return vocab, tokens, rules\n",
    "        \n",
    "    def train_vocab(self, corpus, vocab_size):\n",
    "        corpus = [unidecode(text).lower() for text in corpus]\n",
    "        vocab = {\"<|pad|>\", \"<|unk|>\"} | set(string.printable.lower())\n",
    "        word_freqs = self.get_word_freqs(corpus)            \n",
    "        tokens = self.get_tokens(word_freqs)\n",
    "        vocab, tokens, rules = self.make_rules(vocab,vocab_size,tokens)\n",
    "\n",
    "        return vocab, tokens, rules\n",
    "    \n",
    "    def __init__(self, corpus, vocab_size, max_len):\n",
    "        vocab, tokens, rules = self.train_vocab(corpus, vocab_size)\n",
    "\n",
    "        self.rules = rules\n",
    "        self.tokens = tokens\n",
    "        self.vocab = {v:i for i,v in enumerate(vocab)}\n",
    "        self.vocab_id = {b:a for a,b in self.vocab.items()}\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        self.pad_token = \"<|pad|>\"\n",
    "        self.unk_token = \"<|unk|>\"\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        texts,\n",
    "        truncation=False,\n",
    "    ):\n",
    "        text = [\n",
    "            unidecode(char).lower()\n",
    "            for text in texts\n",
    "            for char in text\n",
    "        ]\n",
    "     \n",
    "\n",
    "        for rule in self.rules:\n",
    "            newtext = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(text) - 1:\n",
    "                if (text[i], text[i + 1]) == rule:\n",
    "                    newtext.append(f\"{rule[0]}{rule[1]}\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    newtext.append(text[i])\n",
    "                i += 1\n",
    "\n",
    "            if i == len(text) - 1:\n",
    "                newtext.append(text[i])\n",
    "\n",
    "            text = newtext\n",
    "\n",
    "        n = self.max_len\n",
    "        ids = [self.vocab[c] if c in self.vocab else 1 for c in text]\n",
    "        \n",
    "        ids = [\n",
    "            ids[i : i + n]\n",
    "            for i in range(0, len(ids), n)\n",
    "            if not truncation or i + n <= len(ids)\n",
    "        ]\n",
    "\n",
    "        mask1 = [[1] * n] * (len(ids) - 1)\n",
    "        mask2 = [[1] * (len(ids[-1])) + [0] * (n - len(ids[-1]))]\n",
    "        mask = mask1 + mask2\n",
    "\n",
    "        ids[-1] = ids[-1] + [0] * (n - len(ids[-1]))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": ids,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNN = 10\n",
    "epochsN = 3\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"huggingface-course/codeparrot-ds-train\")[\"train\"].shuffle(seed=42).select(range(NNN)),\n",
    "    \"valid\": load_dataset(\"huggingface-course/codeparrot-ds-train\")[\"train\"].shuffle(seed=42).select(range(NNN)),\n",
    "})\n",
    "\n",
    "context_length = 128\n",
    "corpset = list(dataset[\"valid\"][\"content\"])\n",
    "\n",
    "vs = 300\n",
    "tokenizer = Tokenizer(corpset,vs,context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32502bd1-93b7-471d-8d7f-6a45286f3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"this is a string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd3238-abe9-4a75-b9a2-916001aaa3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_id[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e393a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    return outputs\n",
    "\n",
    "dataset_tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset[\"train\"].column_names, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_ctx=context_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184189b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "\n",
    "dataset_tokenized.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True)\n",
    "eval_dataloader = DataLoader(dataset_tokenized[\"valid\"], batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather_for_metrics(outputs.loss))\n",
    "        \n",
    "    loss = torch.mean(torch.tensor(losses))\n",
    "    \n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f257466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = epochsN\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 50\n",
    "completed_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53467282",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_fct = CrossEntropyLoss()\n",
    "progress = tqdm.tqdm(total=num_training_steps)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader, start=1):        \n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        \n",
    "        shift_labels = batch[\"input_ids\"][..., 1:].contiguous()\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "    \n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \"\"\"\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        del logits\n",
    "        del shift_logits\n",
    "        del shift_labels     \n",
    "\n",
    "        \n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_sched.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "\n",
    "            \n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"x =\"\"\"\n",
    "newt = []\n",
    "\n",
    "for _ in range(20):\n",
    "    toks = tokenizer(text)\n",
    "    toks = {a:torch.tensor(b).to(\"cuda\") for a,b in toks.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**toks).logits\n",
    "    best_id = torch.argmax(logits, dim=-1)[0][0].item()\n",
    "    print(best_id)\n",
    "    tok = tokenizer.vocab_id[best_id]\n",
    "    newt.append(tok)\n",
    "\n",
    "text = text  + \" \".join(newt)\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
