{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, pipeline\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from accelerate import Accelerator\n",
    "from typing import defaultdict\n",
    "import torch\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def _get_most_freq_pair(self, token_freqs):\n",
    "        pair_freqs = defaultdict(int)\n",
    "\n",
    "        for token, freq in token_freqs.items():\n",
    "            for i in range(len(token) - 1):\n",
    "                pair = token[i : i + 2]\n",
    "                pair_freqs[pair] += freq\n",
    "\n",
    "        return max(pair_freqs.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    def _merge_tokens(self, token_freqs, merge_pair):\n",
    "        token_freqs_new = {}\n",
    "\n",
    "        for token, freq in token_freqs.items():\n",
    "            token_new = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(token):\n",
    "                pair = token[i : i + 2]\n",
    "\n",
    "                if pair == merge_pair:\n",
    "                    token_new.append(\"\".join(pair))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    token_new.append(token[i])\n",
    "                    i += 1\n",
    "\n",
    "            token_new = tuple(token_new)\n",
    "            token_freqs_new[token_new] = freq\n",
    "\n",
    "        return token_freqs_new\n",
    "\n",
    "    def _get_word_freqs(self, texts):\n",
    "        word_freqs = defaultdict(int)\n",
    "\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_freqs[word] += 1\n",
    "\n",
    "        return word_freqs\n",
    "\n",
    "    def _get_token_freqs(self, word_freqs):\n",
    "        token_freqs = {}\n",
    "\n",
    "        for word, freq in word_freqs.items():\n",
    "            token = tuple(word)\n",
    "            token_freqs[token] = freq\n",
    "\n",
    "        return token_freqs\n",
    "\n",
    "    def _pre_tokenize(self, text):\n",
    "        return unidecode(text).lower()\n",
    "\n",
    "    def _build_vocab(self, rules, pad_token, unk_token):\n",
    "        vocab = {pad_token, unk_token}\n",
    "        vocab |= set(self._pre_tokenize(string.printable))\n",
    "        vocab |= {\"\".join(token) for token in rules}\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def _train(self, rules_size, token_freqs):\n",
    "        rules = set()\n",
    "\n",
    "        while len(rules) < rules_size:\n",
    "            pair = self._get_most_freq_pair(token_freqs)\n",
    "            rules.add(pair)\n",
    "            token_freqs = self._merge_tokens(token_freqs, pair)\n",
    "\n",
    "        return rules\n",
    "\n",
    "    def _tokenize(self, texts):\n",
    "        tokens = [char for text in texts for char in self._pre_tokenize(text)]\n",
    "\n",
    "        for merge_pair in self.rules:\n",
    "            tokens_new = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(tokens):\n",
    "                pair = tokens[i : i + 2]\n",
    "\n",
    "                if pair == merge_pair:\n",
    "                    tokens_new.append(\"\".join(pair))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    tokens_new.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "            tokens = tokens_new\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _numericalize(self, tokens):\n",
    "        return [\n",
    "            self.token2id[token] if token in self.token2id else self.unk_token_id\n",
    "            for token in tokens\n",
    "        ]\n",
    "\n",
    "    def _chunk(self, ids):\n",
    "        return [ids[i : i + self.max_len] for i in range(0, len(ids), self.max_len)]\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        padding_amount = self.max_len - len(ids[-1])\n",
    "        ids[-1] += [self.pad_token_id] * padding_amount\n",
    "\n",
    "        return ids, padding_amount\n",
    "\n",
    "    def _get_mask(self, ids, padding_amount):\n",
    "        a = [[1] * self.max_len for _ in range(len(ids) - 1)]\n",
    "        b = [[1] * (self.max_len - padding_amount) + [0] * padding_amount]\n",
    "\n",
    "        return a + b\n",
    "\n",
    "    def __init__(self, texts, rules_size, max_len):\n",
    "        pad_token = \"<|pad|>\"\n",
    "        unk_token = \"<|unk|>\"\n",
    "\n",
    "        texts = [self._pre_tokenize(text) for text in texts]\n",
    "        word_freqs = self._get_word_freqs(texts)\n",
    "        token_freqs = self._get_token_freqs(word_freqs)\n",
    "        rules = self._train(rules_size, token_freqs)\n",
    "        vocab = self._build_vocab(rules, pad_token, unk_token)\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.max_len = max_len\n",
    "        self.rules = rules\n",
    "        self.token2id = {token: i for i, token in enumerate(vocab)}\n",
    "        self.id2token = {i: token for i, token in enumerate(vocab)}\n",
    "        self.pad_token_id = self.token2id[self.pad_token]\n",
    "        self.unk_token_id = self.token2id[self.unk_token]\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        tokens = self._tokenize(texts)\n",
    "        ids = self._numericalize(tokens)\n",
    "        ids = self._chunk(ids)\n",
    "        ids, padding_amount = self._pad(ids)\n",
    "        mask = self._get_mask(ids, padding_amount)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": ids,\n",
    "            \"attention_mask\": mask,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def decode(self, inputs):\n",
    "        output = []\n",
    "        \n",
    "        if isinstance(inputs, dict):\n",
    "            inputs = [input_id for input_ids in inputs[\"input_ids\"] for input_id in input_ids]\n",
    "\n",
    "        for input_id in inputs:\n",
    "            if input_id == self.pad_token_id:\n",
    "                continue\n",
    "            if input_id == self.unk_token_id:\n",
    "                token = \"?\"\n",
    "            else:\n",
    "                token = self.id2token[input_id]\n",
    "\n",
    "            output.append(token)\n",
    "\n",
    "        return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNN = 1000\n",
    "epochsN = 3\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"huggingface-course/codeparrot-ds-train\")[\"train\"].shuffle(seed=42).select(range(NNN)),\n",
    "    \"valid\": load_dataset(\"huggingface-course/codeparrot-ds-train\")[\"train\"].shuffle(seed=42).select(range(NNN)),\n",
    "})\n",
    "\n",
    "context_length = 128\n",
    "corpset = list(dataset[\"valid\"][\"content\"])\n",
    "\n",
    "vs = 3000\n",
    "tokenizer = Tokenizer(corpset,vs,context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e393a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    lambda x: tokenizer(x[\"content\"]),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184189b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "\n",
    "dataset_tokenized.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True)\n",
    "eval_dataloader = DataLoader(dataset_tokenized[\"valid\"], batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather_for_metrics(outputs.loss))\n",
    "        \n",
    "    loss = torch.mean(torch.tensor(losses))\n",
    "    \n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f257466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = epochsN\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 50\n",
    "completed_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53467282",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_fct = CrossEntropyLoss()\n",
    "progress = tqdm.tqdm(total=num_training_steps)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader, start=1):        \n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        \n",
    "        shift_labels = batch[\"input_ids\"][..., 1:].contiguous()\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "    \n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \"\"\"\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        del logits\n",
    "        del shift_logits\n",
    "        del shift_labels     \n",
    "\n",
    "        \n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_sched.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "\n",
    "            \n",
    "        progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"x =\"\"\"\n",
    "newt = []\n",
    "\n",
    "for _ in range(20):\n",
    "    toks = tokenizer(text)\n",
    "    toks = {a:torch.tensor(b).to(\"cuda\") for a,b in toks.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**toks).logits\n",
    "    output = torch.argmax(logits, dim=-1)[0][0].item()\n",
    "    newt.append(output)\n",
    "\n",
    "text += tokenizer.decode(newt)\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
