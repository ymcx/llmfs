{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from urllib import request\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.nn import (\n",
    "    Linear,\n",
    "    Embedding,\n",
    "    ModuleList,\n",
    "    Module,\n",
    "    LayerNorm,\n",
    "    GELU,\n",
    "    ModuleDict,\n",
    "    CrossEntropyLoss,\n",
    ")\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "import pandas\n",
    "import tarfile\n",
    "import tiktoken\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aca821-5116-49cd-9de7-5fe8b3d47c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "\n",
    "        mask = mask.view(mask.size(0), 1, 1, mask.size(1))\n",
    "        att = att.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = GELU(approximate=\"tanh\")\n",
    "        self.c_proj = Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.attn(self.ln_1(x), mask)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size=1024,\n",
    "        vocab_size=50257,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.transformer = ModuleDict(\n",
    "            dict(\n",
    "                wte=Embedding(self.vocab_size, self.n_embd),\n",
    "                wpe=Embedding(self.block_size, self.n_embd),\n",
    "                h=ModuleList([Block(self) for _ in range(self.n_layer)]),\n",
    "                ln_f=LayerNorm(self.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = Linear(self.n_embd, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, mask):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, mask)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc54f54-3f1d-41d1-bed7-ac07d582b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model):\n",
    "    parameters = [\n",
    "        {\"params\": [], \"weight_decay\": WEIGHT_DECAY},\n",
    "        {\"params\": [], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if any(i in name for i in NO_WEIGHT_DECAY):\n",
    "            parameters[1][\"params\"].append(parameter)\n",
    "        else:\n",
    "            parameters[0][\"params\"].append(parameter)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d2b84-0e67-422e-812b-70598edb317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer):\n",
    "    while True:\n",
    "        tokens_and_masks = torch.tensor(tokenize(text), device=\"cuda\")\n",
    "        if tokens_and_masks.shape[0] != 1:\n",
    "            break\n",
    "\n",
    "        length = tokens_and_masks[0, 1].count_nonzero() - 1\n",
    "        tokens = tokens_and_masks[0, 0, :length].unsqueeze(0)\n",
    "        masks = tokens_and_masks[0, 1, :length].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(tokens, masks)[0, -1, :]\n",
    "\n",
    "        probabilities = torch.softmax(logits, dim=0)\n",
    "        probabilities, indices = torch.topk(probabilities, 50)\n",
    "        index = torch.multinomial(probabilities, 1)\n",
    "        token = indices[index]\n",
    "        if token == tokenizer.eot_token:\n",
    "            break\n",
    "\n",
    "        text += tokenizer.decode(token.tolist())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8da146-9576-4500-894e-2757285bf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataloader, loss_fn, accelerator):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for batch, mask in eval_dataloader:\n",
    "        references = batch[..., 1:].contiguous().view(-1)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch, mask)[..., :-1, :].contiguous()\n",
    "            predictions = predictions.view(-1, predictions.size(-1))\n",
    "\n",
    "        loss = loss_fn(predictions, references)\n",
    "        loss = accelerator.gather(loss)\n",
    "        losses.append(loss)\n",
    "\n",
    "    model.train()\n",
    "    loss = sum(losses) / len(losses)\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def eval(step, model, eval_dataloader, train_dataloader, loss_fn, accelerator):\n",
    "    t_loss = 0 # evaluate(model, train_dataloader, loss_fn, accelerator)\n",
    "    e_loss = evaluate(model, eval_dataloader, loss_fn, accelerator)\n",
    "    steps = len(train_dataloader)\n",
    "\n",
    "    s1 = f\"Progress:        {step}/{steps}\"\n",
    "    s2 = f\"Training loss:   {t_loss}\"\n",
    "    s3 = f\"Validation loss: {e_loss}\"\n",
    "\n",
    "    print(s1, s2, s3, sep=\"\\n\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    accelerator,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "):\n",
    "    eval(0, model, eval_dataloader, train_dataloader, loss_fn, accelerator)\n",
    "\n",
    "    for step, (batch, mask) in enumerate(train_dataloader, start=1):\n",
    "        predictions = model(batch, mask)[..., :-1, :].contiguous()\n",
    "        predictions = predictions.view(-1, predictions.size(-1))\n",
    "        references = batch[..., 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = loss_fn(predictions, references) / ACC\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if step % ACC == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if step % INFO_N == 0:\n",
    "            eval(step, model, eval_dataloader, train_dataloader, loss_fn, accelerator)\n",
    "\n",
    "    eval(len(train_dataloader), model, eval_dataloader, train_dataloader, loss_fn, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12054945-ebbb-4aa3-8013-f2911ccf836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df[idx]\n",
    "        tokens = torch.tensor(item[0], dtype=torch.long)\n",
    "        mask = torch.tensor(item[1], dtype=torch.long)\n",
    "        return tokens, mask\n",
    "\n",
    "\n",
    "def parse(string):\n",
    "    string = html.unescape(string)\n",
    "    string = re.sub(r\"<br>\", \"\\n\", string)\n",
    "\n",
    "    while re.search(r\"<[^<>]*>\", string):\n",
    "        string = re.sub(r\"<[^<>]*>\", \"\", string)\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "def download_files():\n",
    "    request.urlretrieve(\n",
    "        \"https://zenodo.org/records/3606810/files/pol_0616-1119_labeled.tar.zst\",\n",
    "        \"pol_0616-1119_labeled.tar.zst\",\n",
    "    )\n",
    "    file = tarfile.open(\"pol_0616-1119_labeled.tar.zst\")\n",
    "    file.extractall()\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    df = pandas.read_json(\"pol_062016-112019_labeled.ndjson\", lines=True, nrows=ROWS)\n",
    "    df = [post[\"com\"] for posts in df[\"posts\"] for post in posts if \"com\" in post]\n",
    "    df = [parse(post) for post in df]\n",
    "    df = [post for post in df if len(post) != 0]\n",
    "    df = [bbb for post in df for bbb in tokenize(post)]\n",
    "\n",
    "    cutoff = int(len(df) * SPLIT)\n",
    "    train = df[:cutoff]\n",
    "    valid = df[cutoff:]\n",
    "\n",
    "    train = PandasDataset(train)\n",
    "    valid = PandasDataset(valid)\n",
    "\n",
    "    train = DataLoader(train, batch_size=BS, shuffle=True)\n",
    "    valid = DataLoader(valid, batch_size=BS, shuffle=False)\n",
    "\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d55dc-b01f-435d-838b-2bfafed7db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    al = []\n",
    "\n",
    "    tokens = tokenizer.encode(text) + [tokenizer.eot_token]\n",
    "    tokens_sublists = [\n",
    "        tokens[i : i + CONTEXT_LENGTH] for i in range(0, len(tokens), CONTEXT_LENGTH)\n",
    "    ]\n",
    "\n",
    "    for tokens_sublist in tokens_sublists:\n",
    "        bbb = []\n",
    "        if len(tokens_sublist) == CONTEXT_LENGTH:\n",
    "            a = tokens_sublist\n",
    "            b = [1] * CONTEXT_LENGTH\n",
    "        else:\n",
    "            m = CONTEXT_LENGTH - len(tokens_sublist)\n",
    "\n",
    "            a = tokens_sublist + [43000] * m\n",
    "            b = [1] * len(tokens_sublist) + [0] * m\n",
    "\n",
    "        al.append([a, b])\n",
    "    return al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c17368-f8fe-4d41-8111-a822a45c9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 128\n",
    "ACC = 32\n",
    "BS = 8\n",
    "LR = 5e-4\n",
    "WARMUP_STEPS = 1000\n",
    "WEIGHT_DECAY = 0.01\n",
    "SPLIT = 0.95\n",
    "INFO_N = 400\n",
    "ROWS = 100\n",
    "\n",
    "NO_WEIGHT_DECAY = [\"wpe.weight\", \"bias\", \"ln\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2b640-0f7e-47a0-ba50-2272f3762c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")\n",
    "\n",
    "train_dl, valid_dl = get_dataloader()\n",
    "\n",
    "model = GPT(block_size=CONTEXT_LENGTH, vocab_size=tokenizer.n_vocab)\n",
    "\n",
    "params = get_parameters(model)\n",
    "\n",
    "optimizer = AdamW(params=params, lr=LR)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, train_dl, valid_dl = accelerator.prepare(\n",
    "    model, optimizer, train_dl, valid_dl\n",
    ")\n",
    "\n",
    "warmup = LinearLR(optimizer, start_factor=LR, total_iters=WARMUP_STEPS)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_dl) - WARMUP_STEPS)\n",
    "lr_scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, scheduler],\n",
    "    milestones=[WARMUP_STEPS],\n",
    ")\n",
    "\n",
    "loss_fn = CrossEntropyLoss(reduction=\"mean\", ignore_index=43000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a553ec-bb2c-453b-9d7f-fab1bd873891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dl, valid_dl, model, loss_fn, accelerator, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb7445-e53a-4af6-a2a9-0e5e67611f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"There\", model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
