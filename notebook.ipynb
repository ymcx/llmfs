{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.nn import Linear, Embedding, ModuleList, Module, LayerNorm, GELU, ModuleDict, CrossEntropyLoss\n",
    "from torch import tril, ones, softmax, long, arange, multinomial, topk, no_grad, tensor\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer\n",
    "from re import sub\n",
    "from math import sqrt\n",
    "from requests import get\n",
    "from pandas import read_parquet\n",
    "from os import mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aca821-5116-49cd-9de7-5fe8b3d47c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            tril(ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = GELU(approximate=\"tanh\")\n",
    "        self.c_proj = Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size=1024,\n",
    "        vocab_size=50257,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.transformer = ModuleDict(\n",
    "            dict(\n",
    "                wte=Embedding(self.vocab_size, self.n_embd),\n",
    "                wpe=Embedding(self.block_size, self.n_embd),\n",
    "                h=ModuleList([Block(self) for _ in range(self.n_layer)]),\n",
    "                ln_f=LayerNorm(self.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = Linear(self.n_embd, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "        pos = arange(0, T, dtype=long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc24d9a-aad5-4d8c-88b6-f45e138cb5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataloader, loss_fn, accelerator):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with no_grad():\n",
    "            logits = model(batch)[..., :-1, :].contiguous()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                batch[..., 1:].contiguous().view(-1),\n",
    "            )\n",
    "        losses.append(accelerator.gather(loss))\n",
    "\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc54f54-3f1d-41d1-bed7-ac07d582b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": 0.1},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d2b84-0e67-422e-812b-70598edb317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, tokenizer, model):\n",
    "    while True:\n",
    "        toks = tokenizer(text, return_tensors=\"pt\", truncation=True)[\"input_ids\"].to(\"cuda\")\n",
    "        with no_grad():\n",
    "            try:\n",
    "                logits = model(toks)[-1][-1]\n",
    "            except AssertionError:\n",
    "                break\n",
    "\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        probs, inds = topk(probs, 50)\n",
    "        selection = inds[multinomial(probs, num_samples=1).item()].item()\n",
    "\n",
    "        if selection == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        text += tokenizer.decode(selection)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8da146-9576-4500-894e-2757285bf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    acc,\n",
    "    accelerator,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    steps,\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"0/{steps}\")\n",
    "        for step, batch in enumerate(train_dataloader, start=1):\n",
    "            logits = model(batch)[..., :-1, :].contiguous()\n",
    "            loss = (\n",
    "                loss_fn(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    batch[..., 1:].contiguous().view(-1),\n",
    "                )\n",
    "                / acc\n",
    "            )\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if step % acc == 0:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"{step}/{steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12054945-ebbb-4aa3-8013-f2911ccf836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return tensor(self.df.iloc[idx][\"posts\"])\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example,\n",
    "        max_length=context_length,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    input_ids = []\n",
    "    for i, length in enumerate(tokens[\"length\"]):\n",
    "        if length != context_length:\n",
    "            continue\n",
    "        token = tokens[\"input_ids\"][i]\n",
    "        input_ids.append(token)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def parse_posts(posts):\n",
    "    posts = [sub(\">>[0-9]{9} {0,1}\", \"\", post[\"content\"]).strip() for post in posts]\n",
    "    return tokenizer.bos_token.join(posts)\n",
    "\n",
    "\n",
    "def download_files():\n",
    "    urls = [\n",
    "        \"https://huggingface.co/datasets/Fal7acy/4chan-archive/resolve/main/data/train-00000-of-00002.parquet?download=true\",\n",
    "        \"https://huggingface.co/datasets/Fal7acy/4chan-archive/resolve/main/data/train-00001-of-00002.parquet?download=true\",\n",
    "    ]\n",
    "    mkdir(\"datasets\")\n",
    "    files = [f\"datasets/{i}.parquet\" for i in range(len(urls))]\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        response = get(url, stream=True)\n",
    "        with open(files[i], \"wb\") as handle:\n",
    "            for data in response.iter_content(chunk_size=1024):\n",
    "                handle.write(data)\n",
    "\n",
    "\n",
    "def get_dataloader(n=None):\n",
    "    df = read_parquet(\"datasets\", engine=\"pyarrow\")\n",
    "    df = df[df[\"board\"] == \"pol\"]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"posts\"] = df[\"posts\"].apply(parse_posts)\n",
    "    df = df.drop([\"board\", \"thread\"], axis=1)\n",
    "    df[\"posts\"] = df[\"posts\"].apply(tokenize)\n",
    "    df = df.explode(\"posts\").reset_index(drop=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    cutoff = int(len(df) * 3 / 4)\n",
    "    train = df.iloc[:cutoff]\n",
    "    valid = df.iloc[cutoff:]\n",
    "\n",
    "    if n!=None:\n",
    "        cutoff = int(len(train) * n)\n",
    "        train = train.iloc[:cutoff]\n",
    "        cutoff = int(len(valid) * n)\n",
    "        valid = valid.iloc[:cutoff]\n",
    "    \n",
    "    train = PandasDataset(train)\n",
    "    valid = PandasDataset(valid)\n",
    "    \n",
    "    train = DataLoader(train, batch_size=bs, shuffle=True)\n",
    "    valid = DataLoader(valid, batch_size=bs, shuffle=False)\n",
    "    \n",
    "    return train,valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c17368-f8fe-4d41-8111-a822a45c9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "epochs = 1\n",
    "acc = 16\n",
    "bs = 16\n",
    "lr = 5e-4\n",
    "warmup = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5fd1e-559c-44af-ac40-42ad3722a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2b640-0f7e-47a0-ba50-2272f3762c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "train_dl,valid_dl = get_dataloader(0.05)\n",
    "\n",
    "model = GPT(block_size=context_length, vocab_size=len(tokenizer))\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=lr)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model, optimizer, train_dl, valid_dl = accelerator.prepare(\n",
    "    model, optimizer, train_dl, valid_dl\n",
    ")\n",
    "\n",
    "steps = epochs * len(train_dl)\n",
    "\n",
    "lr_scheduler1 = LinearLR(optimizer, start_factor=lr, total_iters=warmup)\n",
    "lr_scheduler2 = CosineAnnealingLR(optimizer, T_max=steps - warmup)\n",
    "lr_scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[lr_scheduler1, lr_scheduler2],\n",
    "    milestones=[warmup],\n",
    ")\n",
    "\n",
    "loss_fn = CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045f1fc-a339-4c52-9237-441a50a1d93c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    epochs,\n",
    "    train_dl,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    acc,\n",
    "    accelerator,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a553ec-bb2c-453b-9d7f-fab1bd873891",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model,valid_dl,loss_fn,accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb7445-e53a-4af6-a2a9-0e5e67611f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"There\", tokenizer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
